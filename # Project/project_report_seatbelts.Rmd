---
title: "Bayesian Causal Impact Analysis"
subtitle: "Course - Advanced Topics in Econometrics"
author: "Christoph Bodner"
output: html_notebook
---

Measuring the effect of an intervention on some metric is an important problem in many areas of business and academia. Imagine, you want to know the effect of a recently launched advertising campaign on product sales. In an ideal setting, you would have a treatment and a control group so that you can measure the effect of the campaign. Unfortunately, many times it is not possible or feasible to run a controlled experiment.

This project focuses on settings where we cannot run experiments and still need to estimate the effect of the intervention. To estimate the causal impact, we will try and compare different methods:

- Vanilla Bayesian regression models
- Bayesian count regression models
- Bayesian structural time-series models

The vanilla regression model is as the name suggests, pretty simple: we simply include an indicator variable equal to 1 starting from the time wearing seatbelts became compulsory and check if the corresponding regression coefficient is different from zero. We do the same in the count regression setting. Moreover we also try to model the pre-intervention phase and then forecast the post-intervention phase. Large deviations between the forecast and the actual series can be interpreted as signs that the intervention had an effect (as long as the model had adequate fit in the pre-intervention phase). For the Bayesian structural time-series model we will try to employ the approach used by Brodersen et al (2015) in their paper "Inferring Causal Impact using Bayesian Structural Time-Series Models".

We try to measure the effect that compulsory wearing of seat belts had on UK drivers deaths. We use the 'SeatBelts' dataset that contains information on the monthly total of car drivers in the UK killed or seriously injured between January 1969 and December 1984. Using front-seatbelts became mandatory starting 31 January 1983. 

This project relies mainly on the following packages:
```{r libraries, message=0}
library(dplyr)
library(tibble)
library(timekit)
library(ggplot2)
library(ggthemes)
library(rstan)
library(rstanarm)
```

Now, let's load our data set and take a look at some summary statistics:
```{r load data}
data_seatbelts = tk_tbl(Seatbelts)

data_seatbelts
```
```{r summary}
summary(data_seatbelts)
```

In a next step, we add flags for important safety features that were introduced and split the time index:
```{r data augmentation}
data_seatbelts = data_seatbelts %>%
  mutate(month = lubridate::month(data_seatbelts$index),
         year = lubridate::year(data_seatbelts$index),
         time_index = 1:nrow(data_seatbelts),
         eABS = ifelse(year >= 1978, 1, 0),
         airbag = ifelse(year >= 1981, 1, 0)
         ) %>%
  ## Don't forget to convert dummies to factors! (important for model.matrix())
  mutate_each(funs(as.factor), month, year, law, eABS, airbag)


```


Before we start the analysis, let's take a look at the data:
```{r plot driverskilled}
ggplot(data_seatbelts) +
  geom_rect(xmin = 1983.083, xmax = 1984.917,
          ymin = 0, ymax = 0.02,
          fill = tidyquant::palette_light()[[5]],
          alpha = 0.01) +
  geom_line(aes(x = index, y = DriversKilled/kms)) +
  geom_smooth(aes(x = index, y = DriversKilled/kms), method = "lm") +
  geom_line(data = filter(data_seatbelts, law == 1), aes(x = index, y = DriversKilled/kms), col = "blue") +
  geom_vline(xintercept = c(1978, 1981)) +
  annotate("text", x = 1977.2, y = 0.019, label = "eABS") +
  annotate("text", x = 1980.2, y = 0.019, label = "airbag") +
  annotate("text", x = 1983.2, y = 0.019, label = "seatbelt law") +
  theme_hc() +
  ggtitle("# drivers killed/km driven shows declining trend over whole period", 
          "Blue line highlights compulsory seatbelt law") +
  xlab(NULL)
```
The chart suggests that even before using seatbelts was compulsory, the number of drivers killed per km driven has been declining. So did introducing seatbelts actually help? 

Before we begin to build our models, we split the data into a pre- and post-treatment part:
```{r split data into pre- and post-treatment}
data_pretreat = data_seatbelts %>%
  filter(law != 1)

data_treat = data_seatbelts %>%
  filter(law == 1)
```

We start of by modeling the number of drivers killed using a vanilla regression model:

**! Attention ! data set has rather low number of observations (192) -> quite sensitive to priors!**

### Vanilla regression

For the following hand-coded Gibbs sampler we assume the following model:

- observation equation (aka likelihood): $\textbf{y}|\beta, \sigma \sim \mathcal{N}(\textbf{X}\beta, \sigma^2\textbf{I})$
- prior distributions: $\beta|\sigma^2 \sim \mathcal{N}(\textbf{b}_0, \sigma^2\textbf{B}_0)$, $\sigma^2 \sim \mathcal{IG}(c_0, C_0)$

By Bayes' formula, the posterior density is then given by:
$$p(\beta, \sigma|y) \sim p(\textbf{y}|\beta, \sigma)p(\beta, \sigma^2) = p(\textbf{y}|\beta, \sigma)p(\beta|\sigma^2)p(\sigma^2)$$

Since a normal-inverse-gamma prior is conjugate to a normal likelihood, the posterior is again a normal-inverse-gamma distribution. The conditional distributions are given by:

** insert conditional distributions for beta and sigma **



```{r vanilla regression Gibbs sampler}
X = model.matrix(~ kms + PetrolPrice + VanKilled + law + month, data = data_seatbelts)
y = data_seatbelts$DriversKilled

n = nrow(X)               # no. observations
p = ncol(X)               # no. predictors incl. intercept

draws = 10000
burnin =  1000

## initialize some space to hold results:
res <- matrix(as.numeric(NA), nrow=draws+burnin+1, ncol=p+1)
colnames(res) <- c(paste("beta", 0:(p-1), sep='_'), "sigma2")

## prior values betas
B0 = diag(0.1^2, nrow = p, ncol = p)
b0 = c(180, 0, -500, 0, -10, rep(0, 11))

## prior values sigma
C0 = 1
c0 = 1

## starting values coefficients for sampler
res[1, ] = rep(1, p+1)

## precalculating values
B0_inv = solve(B0)
### (X'X+B0_inv)^(-1)
pre1 = solve(crossprod(X) + B0_inv)

### (X'y + B0_inv*b0)
pre2 = t(X) %*% y + B0_inv %*% b0

### cn = c0 + n/2 + p/2
pre3 = c0 + n/2 + p/2

## sampling steps
for (i in 2:(draws+burnin+1)) {
 if (i%%100 == 0) cat("Iteration", i, "done.\n")
 ## beta sampling
 Bn = res[i-1, p+1] * pre1
 bn = Bn %*% (pre2/res[i-1,p+1])
 ## block draw betas:
 res[i,1:p] <- mvtnorm::rmvnorm(1, bn, Bn)
 
 # draw sigma^2:
 Cn = C0 + .5*(crossprod(y - X %*% res[i, 1:p]) + t(res[i, 1:p] - b0) %*% B0_inv %*% (res[i, 1:p] - b0))
 res[i, p+1] <- 1/rgamma(1, shape = pre3, rate = Cn)
}

## throw away initial value and burn-in:
res = res[-(1:(burnin+1)),]

## posterior mean:
print(colMeans(res))

##OLS estimate:
print(ols <- lm(y ~ X + 0))
```

```{r stan options}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

We use cauchy priors for our betas and an improper uniform prior for sigma with support on $(-\infty, \infty)$.

** Important - model.matrix includes ALL levels of dummies!!! **
```{stan output.var=vanilla_regression}
/*
*Vanilla regression
*/

data {
  int<lower=0> N; //the number of observations
  int<lower=0> K; //the number of columns in the model matrix
  matrix[N,K] X; //the model matrix
  vector[N] y; //response variable
}
parameters {
  vector[K] beta; //regression parameters - column vector by default
  real<lower=0> sigma; //the standard deviation
}
model {  
  beta[1] ~ normal(0, 1000); //prior for the intercept following Gelman 2008
  
  for(i in 2:K)
    beta[i] ~ normal(0, 1000);//prior for the slopes following Gelman 2008

  y ~ normal(X * beta, sigma);
}
```

```{r stan model fitting}
# Create model matrix for Stan
X = model.matrix(~ kms + PetrolPrice + VanKilled + law + month, data = data_seatbelts)
head(as.tibble(X))
y = data_seatbelts$DriversKilled
N = nrow(data_seatbelts)
K = ncol(X)

model_vanilla_test = sampling(vanilla_regression, data = list(N = N, K = K, y = y, X = X),
                              chains = 2, iter = 10000, seed = 20170711, cores = 2, control = list(max_treedepth = 10))
```

```{r diagnostics}
pairs(model_vanilla_test)
summary(model_vanilla_test)
```


```{r vanilla regression}
model_glm_vanilla = stan_glm(formula = DriversKilled ~ kms + PetrolPrice + VanKilled + law + month,
                      data = data_seatbelts,
                      family = gaussian,
                      prior = cauchy(0,2.5),
                      prior_intercept = cauchy(0,10),
                      chains = 2,
                      cores = 2,
                      seed = 20170711
                      )

summary(model_glm_vanilla)
print(ols <- lm(DriversKilled ~ kms + PetrolPrice + VanKilled + law + month, data = data_seatbelts)) # same result as in lm
```



### Poisson regression

```{r rstan glm poisson, message=0}
model_glm = stan_glm(formula = DriversKilled ~ PetrolPrice + VanKilled + month + year, offset = log(kms),
                      data = data_pretreat,
                      family = "poisson",
                      prior = cauchy(0,2.5),
                      prior_intercept = cauchy(0,10),
                      chains = 5,
                      cores = 2,
                      seed = 20170711
                      )

summary(model_glm)
```


### Estimation of counts with Poisson regression

### Sensitivity analysis
#### Likelihood + Prior + Posterior plots
#### Different priors

### Shrinkage priors

### Evaluation of predictive performance


### Open
- Trend vs. differences?
- seasonal behavior with sine/cosine?
- change in petrol price?
