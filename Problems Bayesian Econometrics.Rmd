---
title: "Problems Bayesian Econometrics"
author: "Christoph Bodner"
output:
  html_notebook: default
  html_document: default
---
```{r setup, include=0, echo=0}
library(dplyr)
```

## Problem 1
Using your favorite software, reproduce Section 3 of [1] numerically when X has n and Y has m categories (choose m, n > 2 to your liking). 

Investigate: How does the choice of $p_i$, $i \in \{1, \dots , m \times n\}$ influence the speed of convergence? Can you manage to find values for $\{p_i\}$ that define a joint distribution but “break” the Gibbs sampler?

---

For ease of notation, we take n = m = 3. 

We know that:

$$P(X_k' = x_1 | X_0' = x_0) = A_{X|X}^k$$
with $A_{X|X}^k$ given by:

$$A_{X|X} = 
\underbrace{\begin{pmatrix}
  \frac{p_{1,1}}{\sum_i p_{i,1}} & \frac{p_{2,1}}{\sum_i p_{i,1}} & \frac{p_{3,1}}{\sum_i p_{i,1}} \\
  \frac{p_{1,2}}{\sum_i p_{i,2}} & \frac{p_{2,2}}{\sum_i p_{i,2}} & \frac{p_{3,2}}{\sum_i p_{i,2}} \\
  \frac{p_{1,3}}{\sum_i p_{i,3}} & \frac{p_{2,3}}{\sum_i p_{i,3}} & \frac{p_{3,3}}{\sum_i p_{i,3}}
 \end{pmatrix}}_{=A_{Y|X}}
 \times
 \underbrace{\begin{pmatrix}
  \frac{p_{1,1}}{\sum_i p_{1,i}} & \frac{p_{1,2}}{\sum_i p_{1,i}} & \frac{p_{1,3}}{\sum_i p_{1,i}} \\
  \frac{p_{2,1}}{\sum_i p_{2,i}} & \frac{p_{2,2}}{\sum_i p_{2,i}} & \frac{p_{2,3}}{\sum_i p_{2,i}} \\
  \frac{p_{3,1}}{\sum_i p_{3,i}} & \frac{p_{3,2}}{\sum_i p_{3,i}} & \frac{p_{3,3}}{\sum_i p_{3,i}}
 \end{pmatrix}}_{=A_{X|Y}}$$

Let's define the following conditional distributions:
$$P(X=x|Y=y) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } y = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } y = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } y = 3 \\
  \end{cases}$$

and $P(Y|X) = P(Y) = (1/3, 1/3, 1/3)$, i.e. Y does not depend on X.

Since $P(X) = \mathbb{E}(P(X=x|Y=y))$ we get $P(X) =$ `r paste( round(c((0.2+0.4+0.6)/3, (0.2+0.4+0.2)/3, (0.6+0.2+0.2)/3),2))`.

This is the distribution we should arrive at, when we let $k \to \infty$ and calculate $f_0 A_{X|X}^k = f_k \to (0.4, 0.27, 0.33)$

The matrices $A_{Y|X}$ and $A_{X|Y}$ are given by:
```{r}
A_xy = matrix(c(0.2, 0.2, 0.6,
                0.4, 0.4, 0.2,
                0.6, 0.2, 0.2
              ),
              nrow = 3, byrow = 1
              )


A_yx = matrix(c(1/3, 1/3, 1/3,
                1/3, 1/3, 1/3,
                1/3, 1/3, 1/3
                ),
              nrow = 3, byrow = 1
              )

A_xx = A_yx %*% A_xy
```

As we will see, in our particular case, we will converge to the correct solution after one step already, because Y does not depend on X and hence we can extract the marginal distribution of X immediately.

```{r}
A_xx
```

```{r}
A_xx %*% A_xx
```
The choice of the joint distribution $p_i$ has therefore a huge influence on the speed of convergence: 

* Immediated convergence
* Convergence at some speed and 
* No convergence 

are all possible depending on the distribution specified.

Next, we will show an example where convergence takes more than one step:
Define 
$$P(X=x|Y=y) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } y = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } y = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } y = 3 \\
  \end{cases}$$

as before and let 
$$P(Y=y|X=x) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } x = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } x = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } x = 3 \\
  \end{cases}$$
We get for the transition matrix $A_{X|X}$:
```{r}
A_xy = A_xy
A_yx = A_xy

A_xx = A_yx %*% A_xy

A_xx
```
```{r}
A_xx %*% A_xx
```
```{r}
new = A_xx
old = 0
i = 0
while (sum(abs(new-old))>0.001 & i <= 10){
  old = new
  new = old %*% A_xx
  i = i+1
}
print(paste("Converged after", i, "iterations"))

print(paste("Is a density: " ,if(abs(sum(new)/3-1)<0.001) TRUE))

new
```
And last, but not least, we show an example when this method will not converge:

An easy way to achieve that is to make sure that the sequence gets 'stuck' at some point and cannot explore the whole space:
```{r}
A_xy = matrix(c(0, 0, 1,
                0, 1, 0,
                1, 0, 0
              ),
              nrow = 3, byrow = 1
              )


A_yx = matrix(c(0, 0, 1,
                0, 1, 0,
                1, 0, 0
                ),
              nrow = 3, byrow = 1
              )

A_xx = A_yx %*% A_xy

A_xx
```
Since we have the indentity matrix in this particular case, this sequence will not move from its starting value at all and thus can only 'converge' if we happen to pick the correct starting values by chance. This setting corresponds to a joint distribution with 1/3 on the diagonal and 0s elsewhere.

## Problem 2
Implement a naive Gibbs sampler for Example 2 of [1] with $B = /infty$. Show numerically that the Gibbs sampler fails to converge by investigating a few trace plots of the (log of the) draws.

---

Suppose we have two conditional densities given by:
$$\begin{align*}
f(x|y) &= y e^{-yx}, ~~0<x<\infty \\
f(y|x) &= x e^{-xy}, ~~0<y<\infty \\
\end{align*}$$

We can implement a Gibbs sampler as follows:
```{r}
# No. of Gibbs steps
k = 10000

x = rep(1, k)
y = rep(1, k)

set.seed(1234)

for (i in 2:k){
  x[i] = rexp(1, rate = y[i-1])
  ## Note that y[i] already uses the x[i] generated before and NOT x[i-1]
  y[i] = rexp(1, rate = x[i])
}

gibbs_sequence = data.frame(x, y, iter = 1:k)

library(ggplot2)
ggplot(gibbs_sequence) +
  geom_line(aes(x = iter, y = log(x)), col = "red") +
  geom_line(aes(x = iter, y = log(y))) +
  ylab("log(y) - black and log(x) - red") +
  ggtitle("Traceplots",
          "Individual sequences x (red) and y (black) do not converge")
```

## Problem 3
Implement a Gibbs sampler for Example 2 of [1] with $B << \infty$ and use rejection sampling for the conditional draws in each Gibbs step.

### a
The simplest way to do this is to keep drawing from the unrestricted conditional exponential distribution until you obtain a draw that falls within [0,B]. Take a minute to verify that this actually constitutes a proper rejection sampling step with probability of success in {0,1} (thus, the "coin flip"" can be omitted). Count the number of "rejects"" and time your procedure. What happens if B approaches zero? What happens if B gets large?

---

The conditional densities are given by:
$$\begin{align*}
f(x|y) &\propto y e^{-yx}, ~~0<x<B<<\infty \\
f(y|x) &\propto x e^{-xy}, ~~0<y<B<<\infty \\
\end{align*}$$

The restriction to $(0, B)$ ensures that the marginals exist.

We use rejection sampling, meaning we sample from the full distribution and discard all elements that are not in the acceptance region. 

For rejection sampling to work, we need to find an enveloping density $g(x)$ that satisfies $f(x) \leq cg(x)$ for our target density $f(x)$ (here the truncated exponential distribution) for some constant $c$. Setting $c=1$ and $g(x)$ to the unrestricted exponential distribution fulfills both conditions. Since the distributions are exactly the same over the acceptance region and the target is 0 on the rejection region, we can simply accept all draws in the target region and reject all others.

The lower the treshold B, the higher the probability that we will draw an element that is not acceptable and needs to be rejected. So we expect this method to work well the larger B gets and badly the closer B gets to 0 since it takes longer and longer to obtain a draw that is not rejected.

```{r}
gibbs_rejection = function(B, k){
  start = Sys.time()
  x = rep(B, k)
  y = rep(B, k)
  x_rej = 0
  y_rej = 0
  set.seed(1234)
  
  for (i in 2:k){
    x_accept = FALSE
    while(!x_accept) {
        x_rej = x_rej + 1
        x[i] = rexp(1, rate = y[i-1])
        x_accept = x[i] <= B
    }
    y_accept = FALSE
    while(!y_accept) {
        y_rej = y_rej + 1
        y[i] = rexp(1, rate = x[i])
        y_accept = y[i] <= B
    }
  }
  stop = Sys.time()
  # remove starting value =B
  gibbs_sequence = data.frame(x = x[-1], y = y[-1], iter = 1:(k-1))
    
  results = tibble::tibble(data = list(gibbs_sequence), 
                 x_rej = x_rej, y_rej = y_rej, 
                 time = stop - start)

  return(results)
}
```

The results of the algorithm are as follows:
```{r}
if(FALSE){
    gibbs_exp = NULL
  for (i in c(0.1, 1:8, 100)) {
    gibbs_exp = rbind(gibbs_exp, cbind(i, gibbs_rejection(B = i, 1000)))
  }
  save(gibbs_exp, file = "gibbs_exp.Rdata")
} else {
  load("gibbs_exp.Rdata")
}

gibbs_exp
```

First, let's take a look at the simulated densities:
```{r}
trace_exp = gibbs_exp$data[[6]]
trace_exp_melted = tidyr::gather(trace_exp, var, value, 1:2)

ggplot(trace_exp_melted) +
  geom_density(aes(x = value)) +
  facet_grid(var ~ . ) +
  ggtitle(paste0("Estimated densities (B = ",gibbs_exp$i[6],")"))
```

The true marginal distribution is given by:
$$f(x) = \frac{(1-e^{-Bx})/x}{log(B^2)+\Gamma(0,B^2) + \gamma}$$
So the shape of the marginal should roughly look like this (omitting the $\Gamma$ and $\gamma$ terms since they are positive constants that simply increase the denominator):
```{r}
temp_x = seq(from = 0.01, to = 5, by = 0.01)
plot(density((1-exp(-5*temp_x))/temp_x/(log(5^2))), main = "Approximate f(x)")
```

This suggests that our sampler is actually giving to much weight to large values.

The table shows that the lower the treshold B, the higher the rate of rejection and consequently, the sampler needs to run longer before it draws an appropriate sample.

We can also see that clearly in the following plot:
```{r}
gibbs_exp %>%
  filter(i <100) %>%
  ggplot(.) +
    geom_line(aes(x = i, y = log(as.numeric(time)))) +
    geom_smooth(aes(x = i, y = log(as.numeric(time))), method = "loess") +
    ylab("log(time in sec)") + xlab("treshold (B)") +
    ggtitle("Run-time Gibbs sampler with A/R sampling exponential",
            "Lower rejection treshold increases runtime")
```

### b
Now, use a uniform distribution u on [0,B] as your proposal distribution. Take a minute to verify that the density of the target distribution $p(x|y) = C ye^{−xy}\mathbb{1}_{[0,B]}$ with $C = e^{−By}/B$. Consequently, a constant M such that $Mu(x) \geq p(x|y) \forall x \in \mathbb{R}_+$ is given by $M = By/(1-e^{−By})$. The same logic applies for $p(y|x)$. Count the number of "rejects" and time your procedure. What happens if B approaches zero? What happens if B gets large?

Verify the equivalence of both methods by comparing the draws obtained. Furthermore, plot the number of "rejects" of both methods as a function of B.

---

Before we can use the uniform distribution we need to find a constant $M$ so that $Mu(x) \geq p(x|y) \forall x \in \mathbb{R}_+$. Since the exponential distribution declines monotonically as $x \to \infty$ we know that if we find M s.t. $Mu(x) \geq f(0|y)$ we are good to go. The truncated exponential distribution has a density given by:
$$f(x|y) = y e^{-yx}\mathbb{1}_{[0,B]}$$
We get that:
$$f(0|y) = \frac{y}{1-e^{-yB}}$$
Since a uniform distribution on $[0,B]$ has height $1/B$ we get that $M$ needs to satisfy:
$$M \frac{1}{B} \geq \frac{y}{1-e^{-yB}}$$
So $M = \frac{yB}{1-e^{-yB}}$ is sufficient for our purposes.

Now, let's implement a Gibbs sampler with rejection sampling based on a uniform distribution. We proceed in three steps:

1. Sample a point on the x-axis of our proposal distribution (here a uniform one)
2. Sample a point uniformly starting from the point we drew in 1. upwards till we hit the proposal distribution
3. If the point is below our target distribution (here truncated exponential) we except it, else we draw again

```{r}
gibbs_rejection_unif = function(B, k){
  start = Sys.time()
  x = rep(B, k)
  y = rep(B, k)
  x_rej = 0
  y_rej = 0
  set.seed(1234)

  for (i in 2:k){
    x_accept = FALSE
    while(!x_accept) {
        x_rej = x_rej + 1
        ## sample a point from proposal distribution
        x[i] = runif(1, min = 0, max = B)
        ## sample a point uniformly upwards between [0, M/B]
        x_check = runif(1, 0, y[i-1]*B/(1 - exp(-y[i-1]*B))) 
        ## check if this point is below our target density at point x[i]
        x_accept = x_check <= y[i-1]*exp(-y[i-1]*x[i])
    }
    y_accept = FALSE
    while(!y_accept) {
        y_rej = y_rej + 1
        y[i] = runif(1, min = 0, max = B)
        y_check = runif(1, 0, x[i]*B/(1 - exp(-x[i]*B))) 
        y_accept = y_check <= x[i]*exp(-x[i]*y[i])
    }
  }
  stop = Sys.time()
  # remove starting value =B
  gibbs_sequence = data.frame(x = x[-1], y = y[-1], iter = 1:(k-1))
    
  results = tibble::tibble(data = list(gibbs_sequence), 
                 x_rej = x_rej, y_rej = y_rej, 
                 time = stop - start)

  return(results)
}
```

Running the algorithm for the same set of values as before gives us:
```{r}
if (TRUE) {
  gibbs_unif = NULL
  for (i in c(0.01, 0.1, 1:8)) {
    gibbs_unif = rbind(gibbs_unif, cbind(i, gibbs_rejection_unif(B = i, 1000)))
  }
  save(gibbs_unif, file = "gibbs_unif.Rdata")
} else {
  load("gibbs_unif.Rdata")
  gibbs_unif
}

gibbs_unif
```

This paragraph was written while waiting for the sampler to finish (after drinking coffee, reading an article in the newspaper and some other stuff. In the end I stopped the sampler and set max(i) = 8). The problem with using the uniform distribution is that as B becomes larger and larger, we need more and more draws to find a value we do not reject. Choosing a 'bad' proposal distribution can have a huge impact on run-time. However, for small values of B the uniform distribution is quite handy, because we only sample on the restricted support of the truncated exponential and not on $\mathbb{R}_+$

```{r}
ggplot(gibbs_unif) +
  geom_line(aes(x = i, y = log(as.numeric(time)))) +
  geom_smooth(aes(x = i, y = log(as.numeric(time))), method = "loess") +
  ylab("log(time in sec)") + xlab("treshold (B)") +
  ggtitle("Run-time Gibbs sampler with A/R sampling uniform",
          "Lower rejection treshold decreases runtime up to a certain point")
```

Comparing runtime of both algorithms gives us:
```{r}
gibbs_exp$gibbs = "gibbs_exp"
gibbs_unif$gibbs = "gibbs_unif"
gibbs_compare = rbind(gibbs_exp, gibbs_unif) %>%
  filter(i %in% c(0.1, 1:8))

ggplot(gibbs_compare) +
    geom_line(aes(x = i, y = log(as.numeric(time)), color = gibbs)) +
    ylab("log(time in sec)") + xlab("treshold (B)") +
    ggtitle("Run-time A/R uniform vs. exponential proposal distribution",
          "Uniform performs better for lower B, exponential for larger B") +
  theme(legend.position="bottom")
```

Do both methods actually give us the same distribution or did we make a mistake? We are going to check one example using a qqplot:
```{r}
x_gibbs_exp = gibbs_exp$data[[2]]$x
x_gibbs_unif = gibbs_unif$data[[3]]$x
qqplot(x_gibbs_exp, x_gibbs_unif, xlab = "Gibbs with exponential for B = 1", ylab = "Gibbs with uniform for B = 1")
```
That looks pretty promising. It seems like we did a good job or twice the same bad one (at least for B = 1:)
Comparing the sum of the ordered pair-wise differences gives `r round(sum(sort(x_gibbs_exp) - sort(x_gibbs_unif)),2)` so the draws are as close as the plot suggests.

## Problem 4
Implement a Gibbs sampler for Example 2 of [1] with $B << \infty$ and use a Metropolis sampler for the conditional draws in each Gibbs step (sometimes this is called Metropolis-within-Gibbs).

### a
Consider a symmetric random walk proposal with uniform innovations $J(\theta^*|theta^{(s)} = Unif(\theta^{(s)} - \delta, \theta^{(s)} + \delta)$

---

We are again given two truncated exponential distributions to model our conditional distributions:
$$\begin{align*}
f(x|y) &\propto y e^{-yx}, ~~0<x<B<<\infty \\
f(y|x) &\propto x e^{-xy}, ~~0<y<B<<\infty \\
\end{align*}$$


