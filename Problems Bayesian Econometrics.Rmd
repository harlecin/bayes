---
title: "Problems Bayesian Econometrics"
author: "Christoph Bodner"
output:
  html_notebook: default
  html_document: default
---
```{r setup, include=0, echo=0}
library(dplyr)
library(ggplot2)
library(ggthemes)
```

## Problem 1
Using your favorite software, reproduce Section 3 of [1] numerically when X has n and Y has m categories (choose m, n > 2 to your liking). 

Investigate: How does the choice of $p_i$, $i \in \{1, \dots , m \times n\}$ influence the speed of convergence? Can you manage to find values for $\{p_i\}$ that define a joint distribution but “break” the Gibbs sampler?

---

For ease of notation, we take n = m = 3. 

We know that:

$$P(X_k' = x_1 | X_0' = x_0) = A_{X|X}^k$$
with $A_{X|X}^k$ given by:

$$A_{X|X} = 
\underbrace{\begin{pmatrix}
  \frac{p_{1,1}}{\sum_i p_{i,1}} & \frac{p_{2,1}}{\sum_i p_{i,1}} & \frac{p_{3,1}}{\sum_i p_{i,1}} \\
  \frac{p_{1,2}}{\sum_i p_{i,2}} & \frac{p_{2,2}}{\sum_i p_{i,2}} & \frac{p_{3,2}}{\sum_i p_{i,2}} \\
  \frac{p_{1,3}}{\sum_i p_{i,3}} & \frac{p_{2,3}}{\sum_i p_{i,3}} & \frac{p_{3,3}}{\sum_i p_{i,3}}
 \end{pmatrix}}_{=A_{Y|X}}
 \times
 \underbrace{\begin{pmatrix}
  \frac{p_{1,1}}{\sum_i p_{1,i}} & \frac{p_{1,2}}{\sum_i p_{1,i}} & \frac{p_{1,3}}{\sum_i p_{1,i}} \\
  \frac{p_{2,1}}{\sum_i p_{2,i}} & \frac{p_{2,2}}{\sum_i p_{2,i}} & \frac{p_{2,3}}{\sum_i p_{2,i}} \\
  \frac{p_{3,1}}{\sum_i p_{3,i}} & \frac{p_{3,2}}{\sum_i p_{3,i}} & \frac{p_{3,3}}{\sum_i p_{3,i}}
 \end{pmatrix}}_{=A_{X|Y}}$$

Let's define the following conditional distributions:
$$P(X=x|Y=y) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } y = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } y = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } y = 3 \\
  \end{cases}$$

and $P(Y|X) = P(Y) = (1/3, 1/3, 1/3)$, i.e. Y does not depend on X.

Since $P(X) = \mathbb{E}(P(X=x|Y=y))$ we get $P(X) =$ `r paste( round(c((0.2+0.4+0.6)/3, (0.2+0.4+0.2)/3, (0.6+0.2+0.2)/3),2))`.

This is the distribution we should arrive at, when we let $k \to \infty$ and calculate $f_0 A_{X|X}^k = f_k \to (0.4, 0.27, 0.33)$

The matrices $A_{Y|X}$ and $A_{X|Y}$ are given by:
```{r}
A_xy = matrix(c(0.2, 0.2, 0.6,
                0.4, 0.4, 0.2,
                0.6, 0.2, 0.2
              ),
              nrow = 3, byrow = 1
              )


A_yx = matrix(c(1/3, 1/3, 1/3,
                1/3, 1/3, 1/3,
                1/3, 1/3, 1/3
                ),
              nrow = 3, byrow = 1
              )

A_xx = A_yx %*% A_xy
```

As we will see, in our particular case, we will converge to the correct solution after one step already, because Y does not depend on X and hence we can extract the marginal distribution of X immediately.

```{r}
A_xx
```

```{r}
A_xx %*% A_xx
```
The choice of the joint distribution $p_i$ has therefore a huge influence on the speed of convergence: 

* Immediated convergence
* Convergence at some speed and 
* No convergence 

are all possible depending on the distribution specified.

Next, we will show an example where convergence takes more than one step:
Define 
$$P(X=x|Y=y) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } y = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } y = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } y = 3 \\
  \end{cases}$$

as before and let 
$$P(Y=y|X=x) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } x = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } x = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } x = 3 \\
  \end{cases}$$
We get for the transition matrix $A_{X|X}$:
```{r}
A_xy = A_xy
A_yx = A_xy

A_xx = A_yx %*% A_xy

A_xx
```
```{r}
A_xx %*% A_xx
```
```{r}
new = A_xx
old = 0
i = 0
while (sum(abs(new-old))>0.001 & i <= 10){
  old = new
  new = old %*% A_xx
  i = i+1
}
print(paste("Converged after", i, "iterations"))

print(paste("Is a density: " ,if(abs(sum(new)/3-1)<0.001) TRUE))

new
```
And last, but not least, we show an example when this method will not converge:

An easy way to achieve that is to make sure that the sequence gets 'stuck' at some point and cannot explore the whole space:
```{r}
A_xy = matrix(c(0, 0, 1,
                0, 1, 0,
                1, 0, 0
              ),
              nrow = 3, byrow = 1
              )


A_yx = matrix(c(0, 0, 1,
                0, 1, 0,
                1, 0, 0
                ),
              nrow = 3, byrow = 1
              )

A_xx = A_yx %*% A_xy

A_xx
```
Since we have the indentity matrix in this particular case, this sequence will not move from its starting value at all and thus can only 'converge' if we happen to pick the correct starting values by chance. This setting corresponds to a joint distribution with 1/3 on the diagonal and 0s elsewhere.

## Problem 2
Implement a naive Gibbs sampler for Example 2 of [1] with $B = /infty$. Show numerically that the Gibbs sampler fails to converge by investigating a few trace plots of the (log of the) draws.

---

Suppose we have two conditional densities given by:
$$\begin{align*}
f(x|y) &= y e^{-yx}, ~~0<x<\infty \\
f(y|x) &= x e^{-xy}, ~~0<y<\infty \\
\end{align*}$$

We can implement a Gibbs sampler as follows:
```{r}
# No. of Gibbs steps
k = 10000

x = rep(1, k)
y = rep(2, k)

set.seed(1234)

for (i in 2:k){
  x[i] = rexp(1, rate = y[i-1])
  ## Note that y[i] already uses the x[i] generated before and NOT x[i-1]
  y[i] = rexp(1, rate = x[i])
}

gibbs_sequence = data.frame(x, y, iter = 1:k)

library(ggplot2)
ggplot(gibbs_sequence) +
  geom_line(aes(x = iter, y = log(x)), col = "red") +
  geom_line(aes(x = iter, y = log(y))) +
  ylab("log(.)") +
  ggtitle("Traceplots",
          "Individual sequences x (red) and y (black) do not seem to converge to stable distribution") +
  theme_hc()
```

## Problem 3
Implement a Gibbs sampler for Example 2 of [1] with $B << \infty$ and use rejection sampling for the conditional draws in each Gibbs step.

### a
The simplest way to do this is to keep drawing from the unrestricted conditional exponential distribution until you obtain a draw that falls within [0,B]. Take a minute to verify that this actually constitutes a proper rejection sampling step with probability of success in {0,1} (thus, the "coin flip"" can be omitted). Count the number of "rejects"" and time your procedure. What happens if B approaches zero? What happens if B gets large?

---

The conditional densities are given by:
$$\begin{align*}
f(x|y) &\propto y e^{-yx}, ~~0<x<B<<\infty \\
f(y|x) &\propto x e^{-xy}, ~~0<y<B<<\infty \\
\end{align*}$$

The restriction to $(0, B)$ ensures that the marginals exist.

We use rejection sampling, meaning we sample from the full distribution and discard all elements that are not in the acceptance region. 

For rejection sampling to work, we need to find an enveloping density $g(x)$ that satisfies $f(x) \leq cg(x)$ for our target density $f(x)$ (here the truncated exponential distribution) for some constant $c$. Setting $c=1$ and $g(x)$ to the unrestricted exponential distribution fulfills both conditions. Since the distributions are exactly the same over the acceptance region and the target is 0 on the rejection region, we can simply accept all draws in the target region and reject all others.

The lower the treshold B, the higher the probability that we will draw an element that is not acceptable and needs to be rejected. So we expect this method to work well the larger B gets and badly the closer B gets to 0 since it takes longer and longer to obtain a draw that is not rejected.

```{r}
gibbs_rejection = function(B, k){
  start = Sys.time()
  x = rep(B, k)
  y = rep(B, k)
  x_rej = 0
  y_rej = 0
  set.seed(1234)
  
  for (i in 2:k){
    x_accept = FALSE
    while(!x_accept) {
        x_rej = x_rej + 1
        x[i] = rexp(1, rate = y[i-1])
        x_accept = x[i] <= B
    }
    y_accept = FALSE
    while(!y_accept) {
        y_rej = y_rej + 1
        y[i] = rexp(1, rate = x[i])
        y_accept = y[i] <= B
    }
  }
  stop = Sys.time()
  # remove starting value = B
  gibbs_sequence = data.frame(x = x[-1], y = y[-1], iter = 1:(k-1))
    
  results = tibble::tibble(data = list(gibbs_sequence), 
                 x_rej = x_rej, y_rej = y_rej, 
                 time = stop - start)

  return(results)
}
```

The results of the algorithm are as follows:
```{r}
if(FALSE){
  gibbs_exp = NULL
  for (i in c(0.1, 1:8, 100)) {
    gibbs_exp = rbind(gibbs_exp, cbind(i, gibbs_rejection(B = i, 1000)))
  }
  save(gibbs_exp, file = "gibbs_exp.Rdata")
} else {
  load("gibbs_exp.Rdata")
}

gibbs_exp
```

First, let's take a look at the simulated densities:
```{r}
B = 5

trace_exp = gibbs_exp$data[[6]]
trace_exp = trace_exp 

trace_exp = trace_exp %>%
  rowwise() %>%
  mutate(x_dens = mean(1/(1-exp(-B*.$y))*.$y*exp(-x*.$y)),
         y_dens = mean(1/(1-exp(-B*.$x))*.$x*exp(-y*.$x))
         )
  
trace_exp_melted = tidyr::gather(trace_exp, var, value, 1:2)

ggplot(trace_exp_melted, aes(x = value, y=0.1*..density..)) +
  geom_histogram(binwidth=0.1, boundary = 0) +
  geom_line(data = filter(trace_exp_melted, var == "y"), aes(x = value, y = y_dens*0.1), col = "red", alpha = 0.5) +
  geom_line(data = filter(trace_exp_melted, var == "x"), aes(x = value, y = x_dens*0.1), col = "red", alpha = 0.5) +
  facet_grid(var ~ . ) +
  ylab("frequency (density x bin width)") + xlab("simulated value") +
  ggtitle("Estimated marginal densities scaled by bin width",
          paste0("B = ", gibbs_exp$i[6]," and chain length = 1000")
          ) +
  theme_few()
```

**Note:** ggplot histograms are by default centered around 0. In our case we have to set `boundary = 0` otherwise the histogram will decrease at bin 0 since half the values are from below 0.

The red lines are the estimated densities using $f(x) = \mathbb{E}[f(x|y)] = \int f(x|y)f(y) dy$. As can be seen from the graphs, this way of estimating seems to perform better. The reason for that is that this estimator incorporates more information for any given point.

The true marginal distribution is given by:
$$f(x) = \frac{(1-e^{-Bx})/x}{log(B^2)+\Gamma(0,B^2) + \gamma}$$
So the shape of the marginal should roughly look like this (omitting the $\Gamma$ and $\gamma$ terms since they are positive constants that simply increase the denominator):
```{r}
temp_x = seq(from = 0.01, to = 5, by = 0.01)
plot(1:500/100, ((1-exp(-5*temp_x))/temp_x/(log(5^2)))*0.1, type = "l", 
     main = "Approximate analytic f(x)", ylab = "frequency", xlab = "simulated values")
```

Looks pretty similar to what we got before, so it seems we did a good job:)

Now let's investigate the performance of the algorithm. The table above shows that the lower the treshold B, the higher the rate of rejection and consequently, the sampler needs to run longer before it draws an appropriate sample.

We can also see that clearly in the following plot:
```{r}
gibbs_exp %>%
  filter(i <100) %>%
  ggplot(.) +
    geom_line(aes(x = i, y = log(as.numeric(time)))) +
    geom_smooth(aes(x = i, y = log(as.numeric(time))), method = "loess") +
    ylab("log(time in sec)") + xlab("treshold (B)") +
    ggtitle("Run-time Gibbs sampler with A/R sampling exponential",
            "Lower rejection treshold increases runtime") +
    theme_hc()
```

### b
Now, use a uniform distribution u on [0,B] as your proposal distribution. Take a minute to verify that the density of the target distribution $p(x|y) = C ye^{−xy}\mathbb{1}_{[0,B]}$ with $C = e^{−By}/B$. Consequently, a constant M such that $Mu(x) \geq p(x|y) \forall x \in \mathbb{R}_+$ is given by $M = By/(1-e^{−By})$. The same logic applies for $p(y|x)$. Count the number of "rejects" and time your procedure. What happens if B approaches zero? What happens if B gets large?

Verify the equivalence of both methods by comparing the draws obtained. Furthermore, plot the number of "rejects" of both methods as a function of B.

---

Before we can use the uniform distribution we need to find a constant $M$ so that $Mu(x) \geq p(x|y) \forall x \in \mathbb{R}_+$. Since the exponential distribution declines monotonically as $x \to \infty$ we know that if we find M s.t. $Mu(x) \geq f(0|y)$ we are good to go. The truncated exponential distribution has a density given by:
$$f(x|y) = y e^{-yx}\mathbb{1}_{[0,B]} \frac{1}{1-e^{-By}}$$
which is the density of the exponential distribution rescaled by the probability of $X \leq B$. So we get that:

$$f(0|y) = \frac{y}{1-e^{-yB}}$$
Since a uniform distribution on $[0,B]$ has height $1/B$ we get that $M$ needs to satisfy:
$$M \frac{1}{B} \geq \frac{y}{1-e^{-yB}}$$
So $M = \frac{yB}{1-e^{-yB}}$ is sufficient for our purposes.

Now, let's implement a Gibbs sampler with rejection sampling based on a uniform distribution. We proceed in three steps:

1. Sample a point on the x-axis of our proposal distribution (here a uniform one)
2. Sample a point uniformly starting from the point we drew in 1. upwards till we hit the proposal distribution
3. If the point is below our target distribution (here truncated exponential) we except it, else we draw again

```{r}
gibbs_rejection_unif = function(B, k){
  start = Sys.time()
  x = rep(B, k)
  y = rep(B, k)
  x_rej = 0
  y_rej = 0
  set.seed(1234)

  for (i in 2:k){
    x_accept = FALSE
    while(!x_accept) {
        x_rej = x_rej + 1
        ## sample a point from proposal distribution
        x[i] = runif(1, min = 0, max = B)
        ## sample a point uniformly upwards between [0, M/B]
        x_check = runif(1, 0, y[i-1]*B/(1 - exp(-y[i-1]*B))) 
        ## check if this point is below our target density at point x[i]
        x_accept = x_check <= y[i-1]*exp(-y[i-1]*x[i])
    }
    y_accept = FALSE
    while(!y_accept) {
        y_rej = y_rej + 1
        y[i] = runif(1, min = 0, max = B)
        y_check = runif(1, 0, x[i]*B/(1 - exp(-x[i]*B))) 
        y_accept = y_check <= x[i]*exp(-x[i]*y[i])
    }
  }
  stop = Sys.time()
  # remove starting value =B
  gibbs_sequence = data.frame(x = x[-1], y = y[-1], iter = 1:(k-1))
    
  results = tibble::tibble(data = list(gibbs_sequence), 
                 x_rej = x_rej, y_rej = y_rej, 
                 time = stop - start)

  return(results)
}
```

Running the algorithm gives us:
```{r}
if (FALSE) {
  gibbs_unif = NULL
  for (i in c(0.01, 0.1, 1:8)) {
    gibbs_unif = rbind(gibbs_unif, cbind(i, gibbs_rejection_unif(B = i, 1000)))
  }
  save(gibbs_unif, file = "gibbs_unif.Rdata")
} else {
  load("gibbs_unif.Rdata")
}

gibbs_unif
```

This paragraph was written while waiting for the sampler to finish (after drinking coffee, reading an article in the newspaper and some other stuff. In the end I stopped the sampler and set max(i) = 8). The problem with using the uniform distribution is that as B becomes larger and larger, we need more and more draws to find a value we do not reject. Choosing a 'bad' proposal distribution can have a huge impact on run-time. However, for small values of B the uniform distribution is quite handy, because we only sample on the restricted support of the truncated exponential and not on $\mathbb{R}_+$

```{r}
ggplot(gibbs_unif) +
  geom_line(aes(x = i, y = log(as.numeric(time)))) +
  geom_smooth(aes(x = i, y = log(as.numeric(time))), method = "loess") +
  ylab("log(time in sec)") + xlab("treshold (B)") +
  ggtitle("Run-time Gibbs sampler with A/R sampling uniform",
          "Lower rejection treshold decreases runtime up to a certain point") +
  theme_hc()
```

Comparing runtimes of both algorithms gives us:
```{r}
gibbs_exp$gibbs = "gibbs_exp"
gibbs_unif$gibbs = "gibbs_unif"
gibbs_compare = rbind(gibbs_exp, gibbs_unif) %>%
  filter(i %in% c(0.1, 1:8))

ggplot(gibbs_compare) +
    geom_line(aes(x = i, y = log(as.numeric(time)), color = gibbs)) +
    ylab("log(time in sec)") + xlab("treshold (B)") +
    ggtitle("Run-time A/R uniform vs. exponential proposal distribution",
          "Uniform performs better for lower B, exponential for larger B") +
  theme(legend.position="bottom") +
  theme_hc()
```

Do both methods actually give us the same distribution or did we make a mistake? We are going to check one example using a qqplot:
```{r}
x_gibbs_exp = gibbs_exp$data[[2]]$x
x_gibbs_unif = gibbs_unif$data[[3]]$x
qqplot(x_gibbs_exp, x_gibbs_unif, xlab = "Gibbs with exponential for B = 1", ylab = "Gibbs with uniform for B = 1")
```
That looks pretty promising. It seems like we did a good job or twice the same bad one:)
Comparing the sum of the ordered pair-wise differences gives `r round(sum(sort(x_gibbs_exp) - sort(x_gibbs_unif)),2)` so the draws are as close as the plot suggests.

## Problem 4
Implement a Gibbs sampler for Example 2 of [1] with $B << \infty$ and use a Metropolis sampler for the conditional draws in each Gibbs step (sometimes this is called Metropolis-within-Gibbs).

### a
Consider a symmetric random walk proposal with uniform innovations $J(\theta^*|theta^{(s)}) = Unif(\theta^{(s)} - \delta, \theta^{(s)} + \delta)$

---

We are again given two truncated exponential distributions to model our conditional distributions:
$$\begin{align*}
f(x|y) &\propto y e^{-yx}, ~~0<x<B<<\infty \\
f(y|x) &\propto x e^{-xy}, ~~0<y<B<<\infty \\
\end{align*}$$

The Metropolis sampler works as follows:

Suppose we are at position $\theta^{(s)}$, then:

1. Generate a proposal for a move to $\theta^*$ with probability $J(\theta^*|\theta^{(s)})$
2. Calculate the acceptance ratio $R = min(1, r)$: $$r = \frac{f(\theta^*|y)}{f(\theta^{(s)}|y)} = \frac{f(y|\theta^*)f(\theta^*)}{f(y|\theta^{(s)})f(\theta^{(s)})}$$ 
3. The next draw is given by
$$\theta^{(s+1)} =
  \begin{cases}
  \theta^* & \text{ with probability R}\\
  \theta^{(s)} & \text{ with probability 1-R}\\
\end{cases}$$


```{r}
metropolis_unif = function(B, k, delta = 0.3) {
  x = rep(B, k)
  y = rep(B, k)
  x_temp = 3 
  y_temp = 3
  
  p = function(x, rate, B) {
    rate*exp(-rate*x) * 1/(1-exp(-B*rate)) * (x <= B) * (0 <= x)
  }
  set.seed(1)
  for (i in 1:k) {
    x_proposal = runif(1, min = x_temp - delta, max = x_temp + delta)
    log_r = log(p(x_proposal, rate = y_temp, B)) - log(p(x_temp, rate = y_temp, B))
    accept =  log(runif(1, 0, 1)) < log_r
    x[i] = x_proposal * accept + x_temp * !accept
    x_temp = x[i]
  
    y_proposal = runif(1, min = y_temp - delta, max = y_temp + delta)
    log_r = log(p(y_proposal, rate = x_temp, B)) - log(p(y_temp, rate = x_temp, B))
    accept =  log(runif(1, 0, 1)) < log_r
    y[i] = y_proposal * accept + y_temp * !accept
    y_temp = y[i]
  }
  
  results = tibble::tibble(data = list(data.frame(x = x, y = y)),
                   x_accept_rate = 100 * length(unique(x))/length(x),
                   y_accept_rate = 100 * length(unique(y))/length(y)
                   )

  return(results)
}
  
## Werden die Proposals in Abhängigkeit von allen Parametern gesampelt?
```

Using this sampler gives us:
```{r}
res_unif = NULL
  for (delta in seq(0.1, 0.5, by = 0.1)) {
    res_unif = rbind(res_unif, cbind(delta, metropolis_unif(B = 5, k = 1000, delta)))
  }
res_unif
```

```{r}
metropolis_norm = function(B, k, delta = 0.3) {
  x = rep(B, k)
  y = rep(B, k)
  x_temp = 3 
  y_temp = 3
  
  p = function(x, rate, B) {
    rate*exp(-rate*x) * 1/(1-exp(-B*rate)) * (x <= B) * (0 <= x)
  }
  set.seed(1)
  for (i in 1:k) {
    x_proposal = rnorm(1, mean = x_temp, sqrt(delta))
    log_r = log(p(x_proposal, rate = y_temp, B)) - log(p(x_temp, rate = y_temp, B))
    accept =  log(runif(1, 0, 1)) < log_r
    x[i] = x_proposal * accept + x_temp * !accept
    x_temp = x[i]
  
    y_proposal = rnorm(1, mean = y_temp, sqrt(delta))
    log_r = log(p(y_proposal, rate = x_temp, B)) - log(p(y_temp, rate = x_temp, B))
    accept =  log(runif(1, 0, 1)) < log_r
    y[i] = y_proposal * accept + y_temp * !accept
    y_temp = y[i]
  }
  
  results = tibble::tibble(data = list(data.frame(x = x, y = y)),
                   x_accept_rate = 100 * length(unique(x))/length(x),
                   y_accept_rate = 100 * length(unique(y))/length(y)
                   )

  return(results)
}
  
```

Using this sampler gives us:
```{r}
res_norm = NULL
  for (delta in seq(0.1, 0.5, by = 0.1)) {
    res_norm = rbind(res_norm, cbind(delta, metropolis_norm(B = 5, k = 1000, delta)))
  }
res_norm
```

Comparing with the results form the 'classic' Gibbs samplers from before we get:
```{r}
{
par(mfrow = c(1,2))
qqplot(res_unif$data[[1]]$x, gibbs_exp$data[[6]]$x, main = "Uniform with delta = 0.1")
abline(a = 0, b = 1)
qqplot(res_norm$data[[1]]$x, gibbs_exp$data[[6]]$x) 
abline(a = 0, b = 1)
}

```

Looking at the qqplots, it seems like the uniform Metropolis algorithm does not fit that well. The normal Metropolis one seems to perform better. The uniform Metropolis samples a lot of points in the range 1-3, but comparatively few around 4-5. 

```{r}
{
par(mfrow = c(1,2))
qqplot(res_unif$data[[5]]$x, gibbs_exp$data[[6]]$x)
abline(a = 0, b = 1)
qqplot(res_norm$data[[5]]$x, gibbs_exp$data[[6]]$x) 
abline(a = 0, b = 1)
}

```
When we increase delta, the normal proposal Metropolis performs about on par with the 'classic' Gibbs. The uniform proposal Metropolis shows an even worse performance.

Let's take a look at the autocorrelation of the chains:
```{r}
{
par(mfrow = c(1,3))
acf(res_unif$data[[5]]$x, main = "Unif Metropolis")
acf(res_norm$data[[5]]$x, main = "Norm Metropolis")
acf(gibbs_exp$data[[6]]$x, main = "Classic Gibbs")
}
```
Both Metropolis-within-Gibbs sampler show high degrees of autocorrelation compared to the 'classic' Gibbs sampler. So the sampler is likely to 'walk' in one direction for quite some time.

Since our random variable is restricted to the interval [0,B] and small values are more likely, the sampler is likely to get stuck at values around 0. The acceptance rate of the Metropolis sampler is 1, if the posterior density of the new value is higher than that of the old one. Since the posterior is a truncated exponential, smaller values are less likely to be rejected. But depending on the size of delta, the closer we get to the lower boundary, the more likely it is that we 'overshoot' and draw a value outside of the support. Thus higher probability values are becoming less likely while at the same time it is harder to 'beat' the already quite high posterior to move to less likely values. 


## Problem 5
Implement a Gibbs sampler for Example 2 of [1] with $B << \infty$ and use an independence Metropolis-Hastings sampler for the conditional draws in each Gibbs step (sometimes this is called MH-within-Gibbs). To do so, consider an independence proposal, meaning that the proposed value does not depend on the current value. More specifically:

## a
Try a uniform proposal on $[0, B]$, i.e. $J(\theta^*|\theta^{(s)}) = J(\theta^*) = Unif(0, B)$. Take a minute to verify that the acceptance probabilities simplify to $min[1, p(x^*|y)/p(x^{(s)}|y)]$ and $min[1, p(y^*|x)/p(y^{(s)}|x)]$ respectively.

---

$$\begin{align*}
P(accept) = P(u \leq r) = Unif(0,1) &\leq 
\frac{f(x^*, y^{(s)})}{f(x^{(s)},y^{(s)})}\frac{J_x(x^{(s)}|x^*,y^{(s)})}{J_x(x^{*}|x^{(s)},y^{(s)})} \\
& = \frac{f(x^*|y^{(s)})f(y^{(s)})}{f(x^{(s)}|y^{(s)})f(y^{(s)})}\underbrace{\frac{J_x(x^{(s)})}{J_x(x^{*})}}_{=1/B/1/B=1}
\end{align*}$$

So the acceptance probability is indeed $min[1, p(x^*|y)/p(x^{(s)}|y)]$ for the proposal $x^*$. The same logic applies to the proposal for y.

```{r}
mh_gibbs_ind_unif = function(B, k, delta = 0.3) {
  x = rep(B, k)
  y = rep(B, k)
  x_temp = 3 
  y_temp = 3
  
  p = function(x, rate, B) {
    rate*exp(-rate*x) * 1/(1-exp(-B*rate)) * (x <= B) * (0 <= x)
  }
  set.seed(1)
  for (i in 1:k) {
    x_proposal = runif(1, 0, B)
    log_r = log(p(x_proposal, rate = y_temp, B)) - log(p(x_temp, rate = y_temp, B))
    accept =  log(runif(1, 0, 1)) < log_r
    x[i] = x_proposal * accept + x_temp * !accept
    x_temp = x[i]
  
    y_proposal = runif(1, 0, B)
    log_r = log(p(y_proposal, rate = x_temp, B)) - log(p(y_temp, rate = x_temp, B))
    accept =  log(runif(1, 0, 1)) < log_r
    y[i] = y_proposal * accept + y_temp * !accept
    y_temp = y[i]
  }
  
  results = tibble::tibble(data = list(data.frame(x = x, y = y)),
                   x_accept_rate = 100 * length(unique(x))/length(x),
                   y_accept_rate = 100 * length(unique(y))/length(y)
                   )

  return(results)
}
  
```

Using this sampler gives us:
```{r}
res_ind_unif = NULL
  for (b in c(0.01, 0.1, 1:8)) {
    res_ind_unif = rbind(res_ind_unif, cbind(b, mh_gibbs_ind_unif(B = b, k = 1000)))
  }
res_ind_unif
```

### b
Use the unrestricted conditional exponential distributions $q(x|y) = ye^{-yx}$ and $q(y|x) = xe^{-xy}$. Then the acceptance probabilities on [0,B] are given by:
$$min\left\{1, \frac{p(x^*|y)q(x^{(s)}|y)}{p(x^{(s)}|y)q(x^{*}|y)} \right\}\text{   and   }
min\left\{1, \frac{p(y^*|x)q(y^{(s)}|x)}{p(y^{(s)}|x)q(y^{*}|x)} \right\}$$

```{r}
mh_gibbs_exp = function(B, k, delta = 0.3) {
  x = rep(B, k)
  y = rep(B, k)
  x_temp = 3 
  y_temp = 3
  
  p = function(x, rate, B) {
    rate*exp(-rate*x) * 1/(1-exp(-B*rate)) * (x <= B) * (0 <= x)
  }
  
  set.seed(1)
  for (i in 1:k) {
    x_proposal = rexp(1, rate = y_temp)
    log_r = (log(p(x_proposal, rate = y_temp, B)) + dexp(x_temp, rate = y_temp, log = 1)
             - log(p(x_temp, rate = y_temp, B)) - dexp(x_proposal, rate = y_temp, log = 1))
    accept =  log(runif(1, 0, 1)) < log_r
    x[i] = x_proposal * accept + x_temp * !accept
    x_temp = x[i]
  
    y_proposal = rexp(1, rate = x_temp)
    log_r = (log(p(y_proposal, rate = x_temp, B)) + dexp(y_temp, rate = x_temp, log = 1)
             - log(p(y_temp, rate = x_temp, B)) - dexp(y_proposal, rate = x_temp, log = 1))
    accept =  log(runif(1, 0, 1)) < log_r
    y[i] = y_proposal * accept + y_temp * !accept
    y_temp = y[i]
  }
  
  results = tibble::tibble(data = list(data.frame(x = x, y = y)),
                   x_accept_rate = 100 * length(unique(x))/length(x),
                   y_accept_rate = 100 * length(unique(y))/length(y)
                   )

  return(results)
}
  
```

Using this sampler gives us:
```{r}
res_mh_exp = NULL
  for (b in c(0.01, 0.1, 1:8)) {
    res_mh_exp = rbind(res_mh_exp, cbind(b, mh_gibbs_exp(B = b, k = 1000)))
  }
res_mh_exp
```

Let's check the equivalence of the two approaches for B = 5:
```{r}
{
par(mfrow = c(1, 2)) 
qqplot(res_ind_unif$data[[7]]$x, gibbs_exp$data[[6]]$x, xlab = "MH independent Unif", ylab = "Gibbs Exponential") 
abline(0, 1)
qqplot(res_mh_exp$data[[7]]$x, res_ind_unif$data[[7]]$x, xlab = "MH Exponential", ylab = "MH independent Unif") 
abline(0, 1)
}
```
Using B = 5 yields solid samples with both methods. The sample quantiles align pretty well with the plain Gibbs sampler. From the tables we can see that the independent uniform proposal has decreasing acceptance rates for increasing B while the exponential proposal leads to the reverse. 

ACF plots give the following picture:
```{r}
{
par(mfrow = c(1, 2)) 
acf(res_ind_unif$data[[7]]$x, main = "MH independent Unif")
acf(res_mh_exp$data[[7]]$x, main= "MH Exponential")
}
```

The ACFs decay drastically more quickly than was the case with the vanilla Metropolis algorithm. 

## Problem 6
### a
```{r}
mu_hat_rolling = function(data, start_at = 10) {
  start_at = max(start_at, 2)
  sapply(tail(seq_along(data), -start_at+1), function(n) {mean(data[1:n])})
}

sigma2_hat_rolling = function(data, start_at = 10) {
  start_at = max(start_at, 2)
  sapply(tail(seq_along(data), -start_at+1), function(n) { 1/(n-1)*sum( (data[1:n] - mean(data[1:n]))^2 ) })
}

var_rolling = function(data, alpha = 0.05, start_at = 10) {
  start_at = max(start_at, 2)
  var_alpha = qnorm(p = alpha, mean = mu_hat_rolling(data, start_at), sd = sqrt(sigma2_hat_rolling(data, start_at)))
  exceeded = tail(data, -start_at) < head(var_alpha, -1)
  exceeded
}
```

```{r}
mu = 0
sigma2 = 1
alpha = 0.05
set.seed(12345)

data.sim = rnorm(111, mu, sqrt(sigma2))

sum(var_rolling(data.sim, alpha = 0.05, start_at = 10))/(length(data.sim) - 10)
```

### b
```{r}
if (FALSE) {
  exceeded_sim = replicate(10000, sum(var_rolling(rnorm(111, mu, sqrt(sigma2)), 
                                                  alpha = 0.05, start_at = 10))/(length(data.sim) - 10))
  saveRDS(exceeded_sim, file = "exceeded_sim.Rds")
} else {
  exceeded_sim = readRDS(file = "exceeded_sim.Rds")
}

mean(exceeded_sim)
```
```{r}
hist(exceeded_sim)
```

