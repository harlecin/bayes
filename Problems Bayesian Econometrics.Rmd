---
title: "Problems Bayesian Econometrics"
author: "Christoph Bodner"
output:
  html_notebook: default
  html_document: default
---

## Problem 1
Using your favorite software, reproduce Section 3 of [1] numerically when X has n and Y has m categories (choose m, n > 2 to your liking). 

Investigate: How does the choice of $p_i$, $i \in \{1, \dots , m \times n\}$ influence the speed of convergence? Can you manage to find values for $\{p_i\}$ that define a joint distribution but “break” the Gibbs sampler?

---

For ease of notation, we take n = m = 3. 

We know that:

$$P(X_k' = x_1 | X_0' = x_0) = A_{X|X}^k$$
with $A_{X|X}^k$ given by:

$$A_{X|X} = 
\underbrace{\begin{pmatrix}
  \frac{p_{1,1}}{\sum_i p_{i,1}} & \frac{p_{2,1}}{\sum_i p_{i,1}} & \frac{p_{3,1}}{\sum_i p_{i,1}} \\
  \frac{p_{1,2}}{\sum_i p_{i,2}} & \frac{p_{2,2}}{\sum_i p_{i,2}} & \frac{p_{3,2}}{\sum_i p_{i,2}} \\
  \frac{p_{1,3}}{\sum_i p_{i,3}} & \frac{p_{2,3}}{\sum_i p_{i,3}} & \frac{p_{3,3}}{\sum_i p_{i,3}}
 \end{pmatrix}}_{=A_{Y|X}}
 \times
 \underbrace{\begin{pmatrix}
  \frac{p_{1,1}}{\sum_i p_{1,i}} & \frac{p_{1,2}}{\sum_i p_{1,i}} & \frac{p_{1,3}}{\sum_i p_{1,i}} \\
  \frac{p_{2,1}}{\sum_i p_{2,i}} & \frac{p_{2,2}}{\sum_i p_{2,i}} & \frac{p_{2,3}}{\sum_i p_{2,i}} \\
  \frac{p_{3,1}}{\sum_i p_{3,i}} & \frac{p_{3,2}}{\sum_i p_{3,i}} & \frac{p_{3,3}}{\sum_i p_{3,i}}
 \end{pmatrix}}_{=A_{X|Y}}$$

Let's define the following conditional distributions:
$$P(X=x|Y=y) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } y = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } y = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } y = 3 \\
  \end{cases}$$

and $P(Y|X) = P(Y) = (1/3, 1/3, 1/3)$, i.e. Y does not depend on X.

Since $P(X) = \mathbb{E}(P(X=x|Y=y))$ we get $P(X) =$ `r paste( round(c((0.2+0.4+0.6)/3, (0.2+0.4+0.2)/3, (0.6+0.2+0.2)/3),2))`.

This is the distribution we should arrive at, when we let $k \to \infty$ and calculate $f_0 A_{X|X}^k = f_k \to (0.4, 0.27, 0.33)$

The matrices $A_{Y|X}$ and $A_{X|Y}$ are given by:
```{r}
A_xy = matrix(c(0.2, 0.2, 0.6,
                0.4, 0.4, 0.2,
                0.6, 0.2, 0.2
              ),
              nrow = 3, byrow = 1
              )


A_yx = matrix(c(1/3, 1/3, 1/3,
                1/3, 1/3, 1/3,
                1/3, 1/3, 1/3
                ),
              nrow = 3, byrow = 1
              )

A_xx = A_yx %*% A_xy
```

As we will see, in our particular case, we will converge to the correct solution after one step already, because Y does not depend on X and hence we can extract the marginal distribution of X immediately.

```{r}
A_xx
```

```{r}
A_xx %*% A_xx
```
The choice of the joint distribution $p_i$ has therefore a huge influence on the speed of convergence: 

* Immediated convergence
* Convergence at some speed and 
* No convergence 

are all possible depending on the distribution specified.

Next, we will show an example where convergence takes more than one step:
Define 
$$P(X=x|Y=y) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } y = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } y = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } y = 3 \\
  \end{cases}$$

as before and let 
$$P(Y=y|X=x) =
  \begin{cases}
   p_1 = 0.2, ~p_2 = 0.2,~ p_3 = 0.6 & \text{if } x = 1 \\
   p_1 = 0.4, ~p_2 = 0.4, ~p_3 = 0.2 & \text{if } x = 2 \\
   p_1 = 0.6, ~p_2 = 0.2, ~p_3 = 0.2 & \text{if } x = 3 \\
  \end{cases}$$
We get for the transition matrix $A_{X|X}$:
```{r}
A_xy = A_xy
A_yx = A_xy

A_xx = A_yx %*% A_xy

A_xx
```
```{r}
A_xx %*% A_xx
```
```{r}
new = A_xx
old = 0
i = 0
while (sum(abs(new-old))>0.001 & i <= 10){
  old = new
  new = old %*% A_xx
  i = i+1
}
print(paste("Converged after", i, "iterations"))

print(paste("Is a density: " ,if(abs(sum(new)/3-1)<0.001) TRUE))

new
```
And last, but not least, we show an example when this method will not converge:

An easy way to achieve that is to make sure that the sequence gets 'stuck' at some point and cannot explore the whole space:
```{r}
A_xy = matrix(c(0, 0, 1,
                0, 1, 0,
                1, 0, 0
              ),
              nrow = 3, byrow = 1
              )


A_yx = matrix(c(0, 0, 1,
                0, 1, 0,
                1, 0, 0
                ),
              nrow = 3, byrow = 1
              )

A_xx = A_yx %*% A_xy

A_xx
```
Since we have the indentity matrix in this particular case, this sequence will not move from its starting value at all and thus can only 'converge' if we happen to pick the correct starting values by chance. This setting corresponds to a joint distribution with 1/3 on the diagonal and 0s elsewhere.

## Problem 2
Implement a naive Gibbs sampler for Example 2 of [1] with $B = /infty$. Show numerically that the Gibbs sampler fails to converge by investigating a few trace plots of the (log of the) draws.

---

Suppose we have two conditional densities given by:
$$\begin{align*}
f(x|y) &= y e^{-yx}, ~~0<x<\infty \\
f(y|x) &= x e^{-xy}, ~~0<y<\infty \\
\end{align*}$$

We can implement a Gibbs sampler as follows:
```{r}
# No. of Gibbs steps
k = 10000

x = rep(1, k)
y = rep(1, k)

set.seed(1234)

for (i in 2:k){
  x[i] = rexp(1, rate = y[i-1])
  y[i] = rexp(1, rate = x[i-1])
}

gibbs_sequence = data.frame(x, y, iter = 1:k)

library(ggplot2)
ggplot(gibbs_sequence) +
  geom_line(aes(x = iter, y = log(x)), col = "red") +
  geom_line(aes(x = iter, y = log(y))) +
  ylab("log(y) - black and log(x) - red") +
  ggtitle("Series do not converge",
          "Conditional distributions do not define proper joint distribution")
```

## Problem 3
Implement a Gibbs sampler for Example 2 of [1] with $B << \infty$ and use rejection sampling for the conditional draws in each Gibbs step.

### a
The simplest way to do this is to keep drawing from the unrestricted conditional exponential distribution until you obtain a draw that falls within [0,B]. Take a minute to verify that this actually constitutes a proper rejection sampling step with probability of success in {0,1} (thus, the "coin flip"" can be omitted). Count the number of "rejects"" and time your procedure. What happens if B approaches zero? What happens if B gets large?

### b
Now, use a uniform distribution u on [0,B] as your proposal distribution. Take a minute to verify that the density of the target distribution $p(x|y) = C + ye−xy\mathbb{1}_{[0,B]}$ with $C = e^{−By}/B$. Consequently, a constant M such that $Mu(x) \geq p(x|y) \forall x \in \mathbb{R}_+$ is given by $M = By + e^{−By}$. The same logic applies for $p(y|x)$. Count the number of "rejects" and time your procedure. What happens if B approaches zero? What happens if B gets large?

Verify the equivalence of both methods by comparing the draws obtained. Furthermore, plot the number of "rejects" of both methods as a function of B.

---

The conditional densities are given by:
$$\begin{align*}
f(x|y) &\propto y e^{-yx}, ~~0<x<B<<\infty \\
f(y|x) &\propto x e^{-xy}, ~~0<y<B<<\infty \\
\end{align*}$$

The restriction to $(0, B)$ ensures that the marginals exist.

Let's start with (a). We use rejection sampling, meaning we sample from the full distribution and discard all elements that are not in the acceptance region. The lower the treshold, the higher the probability that we will draw an element that is not acceptable and needs to be rejected. The probability of all possible points in the acceptance region is increased proportionally by the probability of hitting the rejection region.
```{r}
gibbs_rejection = function(B, k){
  start = Sys.time()
  x = rep(B+1, k)
  y = rep(B+1, k)
  x_rej = 0
  y_rej = 0
  set.seed(1234)
  
  for (i in 2:k){
    while(x[i] > B) {
        x_rej = x_rej + 1
        x[i] = rexp(1, rate = y[i-1])
    }
    while(y[i] > B) {
        y_rej = y_rej + 1
        y[i] = rexp(1, rate = x[i-1])
    }
  }
  stop = Sys.time()
  # remove starting value >B
  gibbs_sequence = data.frame(x = x[-1], y = y[-1], iter = 1:(k-1))
    
  results = tibble::tibble(data = list(gibbs_sequence), 
                 x_rej = x_rej, y_rej = y_rej, 
                 time = stop - start)

  return(results)
}
```

The results of the algorithm are as follows:
```{r}
temp = NULL
for (i in c(0.1, 1:8, 100)) {
  temp = rbind(temp, cbind(i, gibbs_rejection(B = i, 1000)))
}
temp
```

The table shows that the lower the treshold B, the higher the rate of rejection and consequently, the sampler needs to run longer before it draws an appropriate sample.

We can also see that clearly in the following plot:
```{r}
ggplot(temp) +
  geom_line(aes(x = 1:10, y = log(as.numeric(time)))) +
  geom_smooth(aes(x = 1:10, y = log(as.numeric(time))), method = "loess") +
  ylab("log(time in sec)") + xlab("treshold (B)") +
  ggtitle("Run-time Gibbs sampler with rejection sampling",
          "Lower rejection treshold increases runtime")
```



